{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import chi2, norm\n",
    "import random\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mtest_1.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mfecha_hora\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      2\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_rows\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Mostrar todas las filas\u001b[39;00m\n\u001b[32m      3\u001b[39m df.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"test_1.csv\", parse_dates=[\"fecha_hora\"])\n",
    "pd.set_option('display.max_rows', None)  # Mostrar todas las filas\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas de Crest Factor (_FC)\n",
    "fc_cols = [col for col in df.columns if col.endswith(\"_FC\")]\n",
    "df = df[[\"fecha_hora\"] + fc_cols]  # Mantener fecha_hora para análisis temporal\n",
    "df = df.set_index('fecha_hora')\n",
    "\n",
    "# Revisar datos faltantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize = (12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos train y test\n",
    "train = df[:'2003-11-04 12:10:00']\n",
    "test = df['2003-11-04 12:00:00':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se normalizan los datos unsando MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(test),  columns=test.columns, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace una reduccion de dimensiones o dimensional para mejorar resultados usando PCA\n",
    "# Se usa n_components=2 pero se usa segun la cantidad de varianza que se quiera conservar\n",
    "pca = PCA(n_components=5, svd_solver= 'full')\n",
    "\n",
    "# Se ajustan los datos \n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "X_train_PCA.index = X_train.index\n",
    "\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "X_test_PCA.index = X_test.index\n",
    "\n",
    "# Se puede ver los componentes principales \n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular varianza acumulada\n",
    "var_acumulada = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Graficar\n",
    "plt.plot(range(1, len(var_acumulada) + 1), var_acumulada, marker='o', linestyle='--')\n",
    "plt.xlabel('Número de componentes')\n",
    "plt.ylabel('Varianza acumulada')\n",
    "plt.title('Varianza acumulada vs. Componentes principales')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La idea es calcular la distancia de Mahalanobis con unos datos de entrenamiento que en este caso seria los datos de funcionamiento normal (los 2 primeros dias para este caso) establecer un umbral, para luego con los demas datos compararlos  poder establecer unas codiciones normales de funcionamiento de la maquina o dispositivo y ver en que momento el funcionamiento ya no es el optimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas son las funciones para calcular la distancia de Mahalanobis\n",
    "def cov_matrix(data, verbose=False):\n",
    "    covariance_matrix = np.cov(data, rowvar=False)\n",
    "    if is_pos_def(covariance_matrix):\n",
    "        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "        if is_pos_def(inv_covariance_matrix):\n",
    "            return covariance_matrix, inv_covariance_matrix\n",
    "        else:\n",
    "            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "    else:\n",
    "        print(\"Error: Covariance Matrix is not positive definite!\") \n",
    "\n",
    "\n",
    "def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "    inv_covariance_matrix = inv_cov_matrix\n",
    "    vars_mean = mean_distr\n",
    "    diff = data - vars_mean\n",
    "    md = []\n",
    "    for i in range(len(diff)):\n",
    "        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "    return md\n",
    "    \n",
    "    \n",
    "def MD_detectOutliers(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    outliers = []\n",
    "    for i in range(len(dist)):\n",
    "        if dist[i] >= threshold:\n",
    "            outliers.append(i)  # index of the outlier\n",
    "    return np.array(outliers)\n",
    "\n",
    "\n",
    "def MD_threshold(dist, extreme=False, verbose=False):\n",
    "    k = 3. if extreme else 2.\n",
    "    threshold = np.mean(dist) * k\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def is_pos_def(A):\n",
    "    if np.allclose(A, A.T):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los datos de train y test usando los resultados obtenidos aplicando PCA\n",
    "data_train = np.array(X_train_PCA.values)\n",
    "data_test = np.array(X_test_PCA.values)\n",
    "\n",
    "# Se calcula la matriz de covarianza y la inversa\n",
    "cov_matrix, inv_cov_matrix  = cov_matrix(data_train)\n",
    "\n",
    "# Se calcula la media de los datos de train\n",
    "mean_distr = data_train.mean(axis=0)\n",
    "\n",
    "# Se calcula la distancia de Mahalanobis para los datos de entrenamiento y se fija el umbral para marcar un punto de datos como anomalo\n",
    "dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "threshold = MD_threshold(dist_train, extreme = True)\n",
    "\n",
    "# Se calcula ahora la distancia para los datos de prueba para compararla con el umbral de anomalia dado \n",
    "dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "\n",
    "print('Calculated threshold value : ' + str(threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como se mencionó anteriormente, el cuadrado de la distancia de Mahalanobis al centroide de la distribución debería seguir una distribución χ² si se cumple el supuesto de que las variables de entrada se distribuyen normalmente. Este es también el supuesto en el que se basa el cálculo anterior del \"valor umbral\" para marcar una anomalía. Dado que es probable que este supuesto no se cumpla en este caso, conviene visualizar la distribución de la distancia de Mahalanobis para establecer un buen valor umbral para marcar anomalías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(np.square(dist_train),\n",
    "             bins = 20, \n",
    "             kde= False,\n",
    "             fit=chi2,\n",
    "             fit_kws={\"label\": \"Chi squared fit\"})\n",
    "ax.set_xlim([0.0,15])\n",
    "ax.set_ylim([0.0,0.4])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se grafica la distribucion de entrenamiento con un ajuste de una distribucion normal\n",
    "# Suponiendo que dist_train es tu conjunto de datos\n",
    "plt.figure()\n",
    "sns.distplot(dist_train,\n",
    "             bins = 10, \n",
    "             kde= True, \n",
    "            color = 'green');\n",
    "plt.xlim([0.0,5])\n",
    "plt.xlabel('Mahalanobis dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un dataframe con los datos de la distancia de mahalanubis de la data de prueba el umbral \n",
    "anomaly = pd.DataFrame()\n",
    "anomaly['Mob dist']= dist_test\n",
    "anomaly['Thresh'] = threshold\n",
    "\n",
    "# Se compara los valores del umbral y de la distancia y se clasifica si es anomalia o no\n",
    "anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "anomaly.index = X_test_PCA.index\n",
    "anomaly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un dataframe con los datos de la distancia de mahalanubis de la data de entrenamiento el umbral \n",
    "anomaly_train = pd.DataFrame()\n",
    "anomaly_train['Mob dist']= dist_train\n",
    "anomaly_train['Thresh'] = threshold\n",
    "\n",
    "# Se compara los valores del umbral y de la distancia y se clasifica si es anomalia o no\n",
    "anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "anomaly_train.index = X_train_PCA.index\n",
    "\n",
    "# Se concatenan los datos de train y test\n",
    "anomaly_alldata = pd.concat([anomaly_train, anomaly])\n",
    "anomaly_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se grafica para ver los resultados \n",
    "anomaly_alldata.plot(logy=True, \n",
    "                     figsize = (12,6), \n",
    "                     ylim = [1e-1,1e3], \n",
    "                     color = ['green','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La idea básica es utilizar la red neuronal Autoencoder para comprimir las lecturas del sensor a una representación de baja dimensión, que captura las correlaciones e interacciones entre las distintas variables. (Esencialmente, el mismo principio que el modelo PCA, pero aquí también se consideran las no linealidades entre las características de entrada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La red del autocodificador se entrena con datos que representan condiciones operativas normales, con el objetivo de comprimir y luego reconstruir las variables de entrada. Durante la reducción de dimensionalidad, la red aprende las interacciones entre las distintas variables y debería ser capaz de reconstruirlas a las variables originales en la salida. La idea es que, a medida que el equipo monitoreado se degrada o se producen anomalías, esto afecte la interacción entre las variables de entrada (por ejemplo, cambios de temperatura, presión, etc.). En este caso, se observará un aumento del error en la reconstrucción de las variables de entrada por parte de la red. Al monitorear el error de reconstrucción, se puede obtener una indicación del estado del equipo monitoreado, ya que este error aumentará a medida que este se degrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "act_func = 'elu'\n",
    "# Input layer:\n",
    "model=Sequential()\n",
    "# First hidden layer, connected to input vector X. \n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.001),\n",
    "                input_shape=(X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "model.add(Dense(2,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(10,activation=act_func,\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "# Train model for 100 epochs, batch size of 10\n",
    "NUM_EPOCHS=50\n",
    "BATCH_SIZE=64\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(np.array(X_train),np.array(X_train),\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.05,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],\n",
    "         'b',\n",
    "         label='Training loss')\n",
    "plt.plot(history.history['val_loss'],\n",
    "         'r',\n",
    "         label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "plt.ylim([0,.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_train))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_train.columns)\n",
    "X_pred.index = X_train.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_train.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(scored['Loss_mae'],\n",
    "            bins=10,\n",
    "            kde=True,\n",
    "            color='blue');\n",
    "plt.title(\"Loss distribution ,training set\")\n",
    "plt.xlim([0.0, .5])  # set y-axis range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = scored['Loss_mae'].quantile(0.25)\n",
    "Q3 = scored['Loss_mae'].quantile(0.75)\n",
    "RIC = Q3 - Q1\n",
    "LS = Q3 + 2 * RIC\n",
    "LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(scored['Loss_mae']) + 4*np.std(scored['Loss_mae'])\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(np.array(X_test))\n",
    "X_pred = pd.DataFrame(X_pred, \n",
    "                      columns=X_test.columns)\n",
    "X_pred.index = X_test.index\n",
    "\n",
    "scored = pd.DataFrame(index=X_test.index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "scored['Threshold'] = thresh\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_train = model.predict(np.array(X_train))\n",
    "X_pred_train = pd.DataFrame(X_pred_train, \n",
    "                      columns=X_train.columns)\n",
    "X_pred_train.index = X_train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=X_train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "scored_train['Threshold'] = thresh\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = pd.concat([scored_train, scored])\n",
    "scored.plot(logy=True,  figsize = (10,6), ylim = [1e-2,1e2], color = ['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
